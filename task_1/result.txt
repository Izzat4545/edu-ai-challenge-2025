I'll work through this database selection problem systematically, showing each step of my reasoning process. Let me start by establishing the foundation and then build up to the final recommendation.
Assumptions and Project Goals
Before diving into the analysis, let me clearly state my assumptions about this social platform:

The platform will have millions of active users (let's estimate 10-50 million for planning purposes)
Users will generate posts frequently (text, images, possibly videos)
The social graph (friend/follow relationships) will be heavily queried for features like news feeds, friend suggestions, and social discovery
Peak usage will create significant load spikes
The system must remain responsive even during rapid user growth
Data consistency is important but not necessarily requiring immediate consistency everywhere

The primary goals are: handle massive scale, optimize for read-heavy workloads, maintain low latency, and support rapid growth.
Step 1: Breaking Down the Requirements
Let me analyze each requirement in detail to understand what we're really dealing with:
Millions of Users: This means we need to think about data volume in the tens to hundreds of terabytes range. User profiles alone might be relatively small (few KB each), but when multiplied by millions of users, plus their posts and relationships, we're looking at substantial storage needs.
Data Models: We have three distinct types of data with different access patterns:

User profiles: Mostly read-heavy, occasionally updated, benefit from fast lookups by user ID
Posts: Write-heavy initially, then read-heavy for popular content, need efficient querying by time, user, and possibly content
Social connections: Graph-like data that needs efficient traversal for friend-of-friend queries, recommendations, and feed generation

80/20 Read-Write Ratio: This is crucial information. It tells us that read optimization should be our primary concern, and we can accept some write complexity if it improves read performance.
Performance and Scalability: Low latency reads are critical, which suggests we need efficient indexing, possibly caching strategies, and horizontal scaling capabilities.
Step 2: Database Categories and Core Characteristics
Let me examine each major database category and their fundamental properties:
Relational SQL Databases (MySQL, PostgreSQL):

Data model: Structured tables with relationships via foreign keys
Consistency: Strong ACID properties
Read performance: Excellent with proper indexing, can struggle with complex joins at scale
Write performance: Good for transactional workloads
Horizontal scalability: Traditionally challenging, though modern solutions like sharding help

Document Stores (MongoDB, CouchDB):

Data model: Flexible JSON-like documents
Consistency: Eventually consistent by default, configurable
Read performance: Very good for single-document reads, flexible querying
Write performance: Excellent, especially for varied data structures
Horizontal scalability: Built-in sharding and replication

Key-Value Stores (Redis, DynamoDB):

Data model: Simple key-value pairs
Consistency: Varies by implementation
Read performance: Extremely fast for direct key lookups
Write performance: Excellent
Horizontal scalability: Excellent with partitioning

Wide-Column Stores (Cassandra, HBase):

Data model: Column families, excellent for time-series and sparse data
Consistency: Tunable consistency levels
Read performance: Excellent for specific access patterns
Write performance: Excellent, optimized for high write throughput
Horizontal scalability: Excellent

Graph Databases (Neo4j, Amazon Neptune):

Data model: Nodes and edges, perfect for relationship data
Consistency: Strong consistency typically
Read performance: Excellent for relationship queries, can struggle with non-graph queries
Write performance: Good
Horizontal scalability: More challenging than other NoSQL options

Step 3: Evaluating Each Category Against Requirements
Now let me work through how each database type would handle our specific social platform needs:
Relational Databases Analysis:
For millions of users, a traditional relational setup would struggle. While user profiles could be stored efficiently in a users table, the social connections would require a many-to-many relationship table that could grow to hundreds of millions of rows. Query performance for friend-of-friend operations would degrade significantly. The 80% read workload could be handled with read replicas, but complex social queries involving multiple joins would create bottlenecks. Sharding would be necessary but complex to implement correctly.
Document Store Analysis:
MongoDB could handle user profiles excellently—each user could be a document with embedded profile data. Posts could also work well as documents. However, the social graph becomes problematic. Storing friend lists as arrays within user documents hits MongoDB's document size limits and makes bidirectional relationship queries inefficient. While reads would be fast for simple queries, complex social graph traversals would require multiple round trips.
Key-Value Store Analysis:
For user profiles, a key-value store like DynamoDB would be excellent—fast lookups by user ID with great scalability. Posts could work with composite keys (user_id + timestamp). However, social relationships become very challenging. You'd need to maintain separate keys for followers and following lists, and complex social queries would require multiple lookups and client-side processing.
Wide-Column Store Analysis:
Cassandra could handle the scale beautifully. User profiles would work well with user_id as the partition key. Posts could be stored efficiently with (user_id, timestamp) as the key for timeline queries. Social connections could be modeled as separate column families. However, complex social graph traversals would still require multiple queries, and strong consistency requirements might be challenging.
Graph Database Analysis:
This is where graph databases shine conceptually. Social connections are naturally represented as graph relationships, making friend-of-friend queries, mutual connections, and recommendation algorithms much more efficient. However, most graph databases struggle with horizontal scaling to millions of users, and storing non-graph data like user profiles and posts feels inefficient.
Step 4: Trade-off Analysis
Let me examine the key trade-offs we face:
Consistency vs. Availability: For a social platform, we can accept eventual consistency for most operations. Friend connections don't need to appear instantly everywhere, and posts can have slight delays in appearing across all feeds.
Schema Flexibility: User profiles and posts might evolve over time, suggesting document or wide-column stores have advantages over rigid relational schemas.
Query Complexity: Social platforms need both simple lookups (user profiles, individual posts) and complex relationship traversals (mutual friends, recommendation algorithms).
Scaling Patterns: We need to think about which pieces scale differently. User profiles grow linearly with users, posts grow much faster, and social connections grow potentially quadratically.
Step 5: Back-of-the-Envelope Calculations
Let me work through some rough numbers to ground this analysis:
For 10 million users:

User profiles: 10M × 5KB = 50GB
Posts (assuming 10 posts per user average): 100M × 2KB = 200GB
Social connections (assuming 200 friends average): 10M × 200 × 2 directions × 16 bytes = 64GB

Read load at 80%: If we assume 1000 requests per second total, that's 800 reads/sec, which most databases can handle. But during peak times, this could be 10-100x higher.
The social graph queries are what will really stress the system—finding mutual friends, generating feeds, making recommendations.
Step 6: Considering Hybrid Approaches
Rather than forcing everything into one database type, let me consider whether a polyglot approach makes sense:
A hybrid architecture could use different databases optimized for different data types:

User profiles: Key-value store for fast lookups
Posts: Document store for flexible content
Social graph: Graph database for relationship queries
Caching layer: Redis for hot data

However, this introduces complexity in maintaining consistency across systems and increases operational overhead.
Final Recommendation
After working through all these considerations, I recommend a Wide-Column Store (specifically Cassandra) as the primary database, with the following reasoning:
Why Cassandra wins:
Scalability: Cassandra was designed for exactly this scale—millions of users with high read/write loads. It handles horizontal scaling seamlessly through its ring architecture.
Performance: With proper data modeling, Cassandra can deliver the low-latency reads required. The 80/20 read-write ratio plays to its strengths.
Data Model Flexibility: We can model all three data types effectively:

User profiles: Partition by user_id for fast lookups
Posts: Partition by user_id, cluster by timestamp for efficient timeline queries
Social connections: Separate table with bidirectional entries for efficient relationship queries

Operational Simplicity: Unlike a polyglot approach, having one primary database reduces operational complexity while still meeting all requirements.
Trade-off Acknowledgment: We sacrifice some query flexibility compared to SQL and some relationship traversal efficiency compared to graph databases, but we gain massive scalability and consistent performance characteristics across all our data types.
Supplementary Components: I'd add Redis as a caching layer for hot user data and frequently accessed social connections, which addresses the remaining performance gaps without architectural complexity.
The key insight driving this decision is that for a social platform at this scale, the ability to handle massive concurrent load while maintaining consistent performance across different data access patterns outweighs the benefits of specialized databases for individual data types. Cassandra provides the best balance of scalability, performance, and operational simplicity for this specific combination of requirements.